---
layout: post
title: 北京号外科技-Python爬虫工程师
categories: 面试题
description: 北京号外科技-Python爬虫工程师
keywords: 爬虫工程师
---
#### 1. 单引号、双引号、三引号的区别？

这几个符号都是可以表示字符串的，如果是表示一行，则用单引号或者双引号表示，它们的区别是:

- 如果内容里有"符号，并且你用双引号表示的话则需要转义字符，而单引号则不需要。

- 三单引号和三双引号也是表示字符串，并且可以表示多行，遵循的是所见即所得的原则。

- 另外，三双引号和三单引号可以作为多行注释来用，单行注释用#号。

#### 2. 如何在一个 function 里面设置一个全局变量？

```
使用 global 进行声明
```

#### 3. 描述 yield 使用场景?

- 生成器

- 当有多个返回值的时候，用return全部裿返回了，需要单个逐一返回时，可以使用 yield

#### 4. 生成 1~10 之间的整数？

```
# 1. 使用 range
for i in range(1, 11)

# 2. 使用生成器
(i for i in range(1, 11))
```

#### 5. Python 如何生成缩略图？

```python
import os
import glob
from PIL import Image

def thumbnail_pic(path):
    a=glob.glob(r'./*.jpg')
    for x in a:
            name=os.path.join(path,x)
        im=Image.open(name)
        im.thumbnail((80,80))
        print(im.format,im.size,im.mode)
        im.save(name,'JPEG')
    print('Done!')

if __name__=='__main__':
    path='.'
    thumbnail_pic(path)
```

#### 6.	列出比较熟悉的爬虫框架，并简要说明？

(1)	Scrapy框架：很强大的爬虫框架，可以满足简单的页面爬取（比如可以明确获知url pattern的情况）。用这个框架可以轻松爬下来如亚马逊商品信息之类的数据。但是对于稍微复杂一点的页面，如weibo的页面信息，这个框架就满足不了需求了。

(2)	Crawley: 高速爬取对应网站的内容，支持关系和非关系数据库，数据可以导出为JSON、XML等

(3)	Portia:可视化爬取网页内容

(4)	newspaper:提取新闻、文章以及内容分析

(5)	python-goose:java写的文章提取工具

(6)	Beautiful Soup:名气大，整合了一些常用爬虫需求。缺点：不能加载JS。

(7)	mechanize:优点：可以加载JS。缺点：文档严重缺失。不过通过官方的example以及人肉尝试的方法，还是勉强能用的。

(8)	selenium:这是一个调用浏览器的driver，通过这个库你可以直接调用浏览器完成某些操作，比如输入验证码。

(9)	cola:一个分布式爬虫框架。项目整体设计有点糟，模块间耦合度较高。

#### 7. 列举常见的反爬技术，并给出应对方案？

##### 1. Headers：

从用户的headers进行反爬是最常见的反爬虫策略。Headers（上一讲中已经提及） 是一种区分浏览器行为和机器行为中最简单的方法，还有一些网站会对 Referer （上级链接）进行检测（机器行为不太可能通过链接跳转实现）从而实现爬虫。

相应的解决措施：通过审查元素或者开发者工具获取相应的headers 然后把相应的headers传输给python的requests，这样就能很好地绕过。

##### 2. IP 限制

一些网站会根据你的IP地址访问的频率，次数进行反爬。也就是说如果你用单一的 IP 地址访问频率过高，那么服务器会在短时间内禁止这个 IP访问。

解决措施：构造自己的 IP 代理池，然后每次访问时随机选择代理（但一些 IP 地址不是非常稳定，需要经常检查更新）。

##### 3.  UA限制

UA是用户访问网站时候的浏览器标识，其反爬机制与ip限制类似。

解决措施：构造自己的UA池，每次python做requests访问时随机挂上UA标识，更好地模拟浏览器行为。当然如果反爬对时间还有限制的话，可以在requests 设置timeout（最好是随机休眠，这样会更安全稳定，time.sleep()）。

##### 4.验证码反爬虫或者模拟登陆
验证码：这个办法也是相当古老并且相当的有效果，如果一个爬虫要解释一个验证码中的内容，这在以前通过简单的图像识别是可以完成的，但是就现在来讲，验证码的干扰线，噪点都很多，甚至还出现了人类都难以认识的验证码。

相应的解决措施：验证码识别的基本方法：截图，二值化、中值滤波去噪、分割、紧缩重排（让高矮统一）、字库特征匹配识别。（python的PIL库或者其他）

模拟登陆（例如知乎等）：用好python requests中的session(下面几行代码实现了最简单的163邮箱的登陆，其实原理是类似的~~）。

```python
import requests
s =requests.session()
login_data={"account":"   ","password":"  "}
res=s.post("http://mail.163.com/",login_data)
```

##### 5.Ajax动态加载

网页的不希望被爬虫拿到的数据使用Ajax动态加载，这样就为爬虫造成了绝大的麻烦，如果一个爬虫不具备js引擎，或者具备js引擎，但是没有处理js返回的方案，或者是具备了js引擎，但是没办法让站点显示启用脚本设置。基于这些情况，ajax动态加载反制爬虫还是相当有效的。

Ajax动态加载的工作原理是：从网页的 url 加载网页的源代码之后，会在浏览器里执行JavaScript程序。这些程序会加载出更多的内容，并把这些内容传输到网页中。这就是为什么有些网页直接爬它的URL时却没有数据的原因。

处理方法：若使用审查元素分析”请求“对应的链接(方法：右键→审查元素→Network→清空，点击”加载更多“，出现对应的GET链接寻找Type为text/html的，点击，查看get参数或者复制Request URL)，循环过程。如果“请求”之前有页面，依据上一步的网址进行分析推导第1页。以此类推，抓取抓Ajax地址的数据。对返回的json使用requests中的json进行解析，使用eval（）转成字典处理（上一讲中的fiddler可以格式化输出json数据。

##### 6.cookie限制

一次打开网页会生成一个随机cookie，如果再次打开网页这个cookie不存在，那么再次设置，第三次打开仍然不存在，这就非常有可能是爬虫在工作了。

解决措施：在headers挂上相应的cookie或者根据其方法进行构造（例如从中选取几个字母进行构造）。如果过于复杂，可以考虑使用selenium模块（可以完全模拟浏览器行为）。

#### 8. 网络协议 http 和 https 区别？

HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从 WWW 服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。

HTTPS：是以安全为目标的 HTTP 通道，简单讲是 HTTP 的安全版，即 HTTP 下加入 SSL 层，HTTPS的安全基础是 SSL，因此加密的详细内容就需要 SSL。

HTTPS 协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。

#### 9. 什么是 cookie，session 有什么区别？

- 1、cookie 数据存放在客户的浏览器上，session 数据放在服务器上。

- 2、cookie 不是很安全，别人可以分析存放在本地的 cookie 并进行 cookie 欺骗，考虑到安全应当使用 session。

- 3、session 会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用 cookie。

- 4、单个 cookie 保存的数据不能超过 4K，很多浏览器都限制一个站点最多保存 20 个 cookie。

- 5、可以考虑将登陆信息等重要信息存放为 session，其他信息如果需要保留，可以放在 cookie 中。

#### 10. Mysql 中 myisam 与 innodb 的区别？

##### 1、 存储结构

- MyISAM：每个 MyISAM 在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型.frm 文件存储表定义。数据文件
的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI(MYIndex)。

- InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），

- InnoDB 表的大小只受限于操作系统文件的大小，一般为 2GB。

##### 2、 存储空间

- MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。

- InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。

##### 3、 事务支持

- MyISAM：强调的是性能，每次查询具有原子性,其执行数度比 InnoDB 类型更快，但是不提供事务支持。

- InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。

##### 4、 CURD 操作

- MyISAM：如果执行大量的 SELECT，MyISAM 是更好的选择。(因为没有支持行级锁)，在增删的时候需要锁定整个表格，效率会低一些。相关的是 innodb 支持行级锁，删除插入的时候只需要锁定改行就行，效率较高

- InnoDB：如果你的数据执行大量的 INSERT 或 UPDATE，出于性能方面的考虑，应该使用 InnoDB表。DELETE 从性能上 InnoDB 更优，但 DELETE FROM table 时，InnoDB 不会重新建立表，而是一行一行的删除，在 innodb 上如果要清空保存有大量数据的表，最好使用truncate table 这个命令。

##### 5、 外键

- MyISAM：不支持

- InnoDB：支持
